{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0765c337-13ee-4ab0-82d3-6fe63893a264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier climat sauvegardé dans : /home/onyxia/work/Data Science/Project/Data/data_climat.csv\n"
     ]
    }
   ],
   "source": [
    "# les données climatiques sont volumineuses (un fichier par département)\n",
    "# pour ne pas les télécharger à la main, on crée un programme pour les importer automatiquement \n",
    "# en sélectionnant uniquement les variables d'intérêt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fonctions import recup_url, filtre_data\n",
    "\n",
    "cols = ['NOM_USUEL', \n",
    "        \"AAAAMM\",\n",
    "        \"TM\", \n",
    "        \"TX\",\n",
    "        \"RR\", \n",
    "        \"UMM\", \n",
    "        \"FFM\", \n",
    "        \"TXMIN\", \n",
    "        \"NBJTX0\", \n",
    "        \"NBJTX25\", \n",
    "        \"NBJTX30\", \n",
    "        \"NBJTX35\",\n",
    "        \"NBJNEIG\", \n",
    "        \"NBJSOLNG\"]\n",
    "\n",
    "cols_indic = cols[2:len(cols)] \n",
    "\n",
    "\n",
    "# pour chaque département, on va procéder de la même façon\n",
    "# on crée donc une fonction qui prend le département comme argument\n",
    "\n",
    "def agreg_dpt(DEP):\n",
    "    df_filtre = recup_url.url_to_df(url = \"https://object.files.data.gouv.fr/meteofrance/data/synchro_ftp/BASE/MENS/MENSQ_\" + DEP + \"_previous-1950-2023.csv.gz\",\n",
    "                        cols_a_conserver=cols,\n",
    "                        type_zip=\"gz\",\n",
    "                        plusieurs_fichiers=False)\n",
    "# on crée une variable ne contenant que l'annee \n",
    "    df_filtre['AAAA'] = df_filtre['AAAAMM'].astype(str).str[:4].astype(int)\n",
    "    df_filtre['MM']= df_filtre['AAAAMM'].astype(str).str[-2:].astype(int)\n",
    "# on sélectionne nos mois et années d'intérêt\n",
    "    df_filtre = filtre_data.filtre_annee_mois(df_filtre)\n",
    "# on calcule la moyenne départementale pour toutes les variables\n",
    "    df_filtre['TX_num']=df_filtre.TX.astype(\"float64\")\n",
    "    df_filtre['TXMIN_num']=df_filtre.TXMIN.astype(\"float64\")\n",
    "    df_filtre = df_filtre.groupby(['AAAA','MM'])[cols_indic].mean()\n",
    "    df_filtre['DEP'] = DEP\n",
    "    return(df_filtre)\n",
    "\n",
    "# on prépare une boucle pour importer un à un les fichiers départements\n",
    "# et concaténer les outputs de la fonction agreg_dpt\n",
    "liste_dep = list(np.arange(2,96))\n",
    "df = agreg_dpt(\"01\")\n",
    "for i in liste_dep:\n",
    "    num = f'{i:02}'\n",
    "    df = pd.concat([df, agreg_dpt(num)])\n",
    "\n",
    "# on remet année et mois (devenues index) en variables normales\n",
    "df = df.reset_index() \n",
    "\n",
    "df[\"DEP\"] = df.DEP.astype(str)\n",
    "df[\"DEP\"] = df[\"DEP\"].str.zfill(2)\n",
    "\n",
    "\n",
    "# on crée deux departements différents pour la Corse pour pouvoir cartographier ensuite\n",
    "new_rows = df.loc[df['DEP']==\"20\"].copy()\n",
    "new_rows1 = new_rows.copy()\n",
    "new_rows[\"DEP\"] = \"2A\"\n",
    "new_rows1[\"DEP\"] = \"2B\"\n",
    "df = df._append([new_rows,new_rows1])\n",
    "df = df.loc[df[\"DEP\"] != \"20\"]\n",
    "\n",
    "# saison (été ou hiver)\n",
    "conditions1 = [\n",
    "    (df['MM'] <= 3) | (df['MM'] == 12),\n",
    "    (df['MM'] >= 6) & (df['MM'] <= 9)\n",
    "    ]\n",
    "\n",
    "values1 = ['hiver', 'été']\n",
    "\n",
    "df['saison'] = np.select(conditions1, values1, default='Other')\n",
    "\n",
    "# période (avant ou après 2015)\n",
    "conditions2 = [\n",
    "    (df['AAAA'] <= 2015),\n",
    "    (df['AAAA'] > 2015)\n",
    "    ]\n",
    "\n",
    "values2 = ['avant_2015', 'apres_2015']\n",
    "\n",
    "df['periode'] = np.select(conditions2, values2, default='Other')\n",
    "\n",
    "base_temp = df\n",
    "base_temp.head()\n",
    "\n",
    "\n",
    "# On veut ajouter la nouvelle base de données, que l'on vient de créer dans dossier Data\n",
    "from pathlib import Path\n",
    "\n",
    "# dossier racine du projet = 3 niveaux au-dessus de ce fichier\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().parents[1]\n",
    "\n",
    "data_dir = PROJECT_ROOT / \"Data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = data_dir / \"data_climat.csv\"\n",
    "base_temp.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Fichier climat sauvegardé dans :\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741186b1-09b7-4ef3-bb09-09465638a8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier tourisme sauvegardé dans : /home/onyxia/work/Data Science/Project/Data/data_tourisme.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fonctions import recup_url, filtre_data\n",
    "\n",
    "cols = [\n",
    "    'ACTIVITY',\n",
    "    'FREQ', \n",
    "    'GEO', \n",
    "    'GEO_OBJECT',\n",
    "    'TERRTYPO',\n",
    "    'TOUR_MEASURE',\n",
    "    'TOUR_RESID',\n",
    "    'CONF_STATUS',\n",
    "    'DECIMALS',\n",
    "    'OBS_STATUS',\n",
    "    'OBS_STATUS_FR',\n",
    "    'UNIT_MULT',\n",
    "    'TIME_PERIOD',\n",
    "    'OBS_VALUE'\n",
    "]\n",
    "\n",
    "\n",
    "#df = pd.read_csv(\"s3://machevallier/DS_TOUR_FREQ_data.csv\", sep=';', usecols=cols)\n",
    "\n",
    "df = recup_url.url_to_df(url = \"https://www.data.gouv.fr/api/1/datasets/r/1129fd80-2564-452c-86d4-9e36e7cca4a5\",\n",
    "                          cols_a_conserver=cols,\n",
    "                          type_zip=\"zip\",\n",
    "                          plusieurs_fichiers=True)\n",
    "\n",
    "# on applique le facteur d'échelle \n",
    "df[\"OBS_VALUE_CORR\"] = df[\"OBS_VALUE\"] * (10 ** df[\"UNIT_MULT\"])\n",
    "\n",
    "# on met le bon nombre de décimales\n",
    "df.loc[df[\"OBS_VALUE_CORR\"].notna(), \"OBS_VALUE_CORR\"] = df.loc[df[\"OBS_VALUE_CORR\"].notna()].apply(\n",
    "    lambda x: round(x[\"OBS_VALUE_CORR\"], int(x[\"DECIMALS\"])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# on garde seulement la valeur observée qui est corrigée\n",
    "df = df.drop(columns=[\"DECIMALS\", \"UNIT_MULT\", \"OBS_VALUE\"])\n",
    "\n",
    "# on choisit le nombre d'arrivée comme indicateur\n",
    "df = df.loc[df['TOUR_MEASURE'].isin([\"ARR\"])]\n",
    "\n",
    "#df['ACTIVITY'] = df['ACTIVITY'].replace(['I553'],['CAMPING'])\n",
    "\n",
    "#df = df[df[\"ACTIVITY\"] == \"CAMPING\"]\n",
    "\n",
    "df = df.loc[df['OBS_STATUS'].isin([\"A\", \"P\"])]\n",
    "# on exclut les valeurs manquantes (O), A= Normale (définitive/validée), P= Valeur provisoire\n",
    "# en faisant: print(df[\"OBS_STATUS\"].value_counts(dropna=False))\n",
    "# on obtient OBS_STATUS; A 69627; Name: count, dtype: int64\n",
    "# il n'y a donc pas de P, on va pouvoir supprimer la colonne OBS_STATUS\n",
    "df = df.drop(columns=[\"OBS_STATUS\"])\n",
    "\n",
    "# on remarque que certaines valeurs définitives sont marquées Prov sous OBS_STATUS_FR\n",
    "# A = valeur correcte du point de vue technique,\n",
    "# mais OBS_STATUS_FR = \"PROV\" = pas encore consolidée statistiquement.\n",
    "# en faisant: print(df[\"CONF_STATUS\"].value_counts(dropna=False))\n",
    "# on obtient CONF_STATUS; F 69627; Name: count, dtype: int64\n",
    "# il n'y a donc que des observations diffusables, on va pouvoir supprimer la colonne CONF_STATUS\n",
    "df = df.drop(columns=[\"CONF_STATUS\",\"TOUR_MEASURE\",\"OBS_STATUS_FR\"])\n",
    "\n",
    "# On prend les données mensuelles et on supprime la colonne FREQ\n",
    "df = df.loc[df['FREQ'].isin([\"M\"])]\n",
    "df = df.drop(\"FREQ\", axis = 1)\n",
    "\n",
    "# On définit les années que l'on veut garder\n",
    "# on crée une variable ne contenant que l'annee et une autre le mois\n",
    "df['AAAA'] = df['TIME_PERIOD'].astype(str).str[:4].astype(int)\n",
    "df['MM']= df['TIME_PERIOD'].astype(str).str[5:7].astype(int)\n",
    "df = df.drop(\"TIME_PERIOD\", axis = 1)\n",
    "\n",
    "#on filtre les données sur nos mois d'interet\n",
    "df = filtre_data.filtre_annee_mois(df)\n",
    "\n",
    "# On prend les données de departement et on supprime c\n",
    "df = df.loc[df['GEO_OBJECT'].isin([\"DEP\"])]\n",
    "df = df.drop(\"GEO_OBJECT\", axis = 1)\n",
    "\n",
    "# on supprime TERRTYPO car tout est identique\n",
    "df = df.drop(\"TERRTYPO\", axis = 1)\n",
    "\n",
    "# on exclut les DOM TOM\n",
    "df = df.loc[df['GEO'].str.len() == 2]\n",
    "\n",
    "DEP_NOM = {\n",
    "    \"01\": \"Ain\", \"02\": \"Aisne\", \"03\": \"Allier\", \"04\": \"Alpes-de-Haute-Provence\",\n",
    "    \"05\": \"Hautes-Alpes\", \"06\": \"Alpes-Maritimes\", \"07\": \"Ardèche\", \"08\": \"Ardennes\",\n",
    "    \"09\": \"Ariège\", \"10\": \"Aube\", \"11\": \"Aude\", \"12\": \"Aveyron\", \"13\": \"Bouches-du-Rhône\",\n",
    "    \"14\": \"Calvados\", \"15\": \"Cantal\", \"16\": \"Charente\", \"17\": \"Charente-Maritime\",\n",
    "    \"18\": \"Cher\", \"19\": \"Corrèze\", \"21\": \"Côte-d'Or\", \"22\": \"Côtes-d'Armor\",\n",
    "    \"23\": \"Creuse\", \"24\": \"Dordogne\", \"25\": \"Doubs\", \"26\": \"Drôme\", \"27\": \"Eure\",\n",
    "    \"28\": \"Eure-et-Loir\", \"29\": \"Finistère\", \"2A\": \"Corse\", \"2B\": \"Corse\",\n",
    "    \"30\": \"Gard\", \"31\": \"Haute-Garonne\", \"32\": \"Gers\", \"33\": \"Gironde\", \"34\": \"Hérault\",\n",
    "    \"35\": \"Ille-et-Vilaine\", \"36\": \"Indre\", \"37\": \"Indre-et-Loire\", \"38\": \"Isère\",\n",
    "    \"39\": \"Jura\", \"40\": \"Landes\", \"41\": \"Loir-et-Cher\", \"42\": \"Loire\", \"43\": \"Haute-Loire\",\n",
    "    \"44\": \"Loire-Atlantique\", \"45\": \"Loiret\", \"46\": \"Lot\", \"47\": \"Lot-et-Garonne\",\n",
    "    \"48\": \"Lozère\", \"49\": \"Maine-et-Loire\", \"50\": \"Manche\", \"51\": \"Marne\",\n",
    "    \"52\": \"Haute-Marne\", \"53\": \"Mayenne\", \"54\": \"Meurthe-et-Moselle\", \"55\": \"Meuse\",\n",
    "    \"56\": \"Morbihan\", \"57\": \"Moselle\", \"58\": \"Nièvre\", \"59\": \"Nord\", \"60\": \"Oise\",\n",
    "    \"61\": \"Orne\", \"62\": \"Pas-de-Calais\", \"63\": \"Puy-de-Dôme\", \"64\": \"Pyrénées-Atlantiques\",\n",
    "    \"65\": \"Hautes-Pyrénées\", \"66\": \"Pyrénées-Orientales\", \"67\": \"Bas-Rhin\",\n",
    "    \"68\": \"Haut-Rhin\", \"69\": \"Rhône\", \"70\": \"Haute-Saône\", \"71\": \"Saône-et-Loire\",\n",
    "    \"72\": \"Sarthe\", \"73\": \"Savoie\", \"74\": \"Haute-Savoie\", \"75\": \"Paris\",\n",
    "    \"76\": \"Seine-Maritime\", \"77\": \"Seine-et-Marne\", \"78\": \"Yvelines\", \"79\": \"Deux-Sèvres\",\n",
    "    \"80\": \"Somme\", \"81\": \"Tarn\", \"82\": \"Tarn-et-Garonne\", \"83\": \"Var\", \"84\": \"Vaucluse\",\n",
    "    \"85\": \"Vendée\", \"86\": \"Vienne\", \"87\": \"Haute-Vienne\", \"88\": \"Vosges\", \"89\": \"Yonne\",\n",
    "    \"90\": \"Territoire de Belfort\", \"91\": \"Essonne\", \"92\": \"Hauts-de-Seine\",\n",
    "    \"93\": \"Seine-Saint-Denis\", \"94\": \"Val-de-Marne\", \"95\": \"Val-d'Oise\",\n",
    "    \"971\": \"Guadeloupe\", \"972\": \"Martinique\", \"973\": \"Guyane\", \"974\": \"La Réunion\",\n",
    "    \"976\": \"Mayotte\"\n",
    "}\n",
    "\n",
    "df.insert(\n",
    "    1,  # position (0 = première colonne, 1 = deuxième, etc.)\n",
    "    \"DEP_NOM\",  # nom de la nouvelle colonne\n",
    "    df[\"GEO\"].map(DEP_NOM)\n",
    ")\n",
    "\n",
    "#changer le nom de la colonne GEO en DEP pour la fusion\n",
    "col = df.columns.tolist()\n",
    "col[0] = \"DEP\"\n",
    "df.columns = col\n",
    "\n",
    "df['DEP'] = df['DEP'].replace(['2A', '2B'],['20', '20'])\n",
    "\n",
    "# la variable \"TOUR_RESID\" donne l'origine du touriste : on filtre sur total (on ne distingue pas pour l'instant)\n",
    "df = df[df[\"TOUR_RESID\"] != \"_T\"]\n",
    "\n",
    "# print(df[\"ACTIVITY\"].value_counts(dropna=False))\n",
    "# Aucun camping n'est présent dans notre sélection\n",
    "\n",
    "df['TOUR_RESID'] = df['TOUR_RESID'].replace(['250', '1_X_250'],['France', 'Étranger'])\n",
    "\n",
    "# on somme les arrivées par année et mois\n",
    "df = df.groupby(['AAAA','MM', 'DEP', 'DEP_NOM', 'TOUR_RESID'])[\"OBS_VALUE_CORR\"].sum()\n",
    "\n",
    "# on remet année et mois (devenues index) en variables normales\n",
    "df = df.reset_index() \n",
    "\n",
    "base_touri = df\n",
    "\n",
    "# On veut mettre notre nouvelle base de données dans Data \n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    PROJECT_ROOT = Path(__file__).resolve().parents[2]\n",
    "except NameError:\n",
    "    PROJECT_ROOT = Path.cwd().parents[1]\n",
    "data_dir = PROJECT_ROOT / \"Data\"\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_path = data_dir / \"data_tourisme.csv\"\n",
    "base_touri.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Fichier tourisme sauvegardé dans :\", output_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9732711c-54e5-4b15-85f7-b0c2a197dae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
